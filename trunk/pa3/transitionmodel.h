#ifndef TRANSITION_MODEL_H
#define TRANSITION_MODEL_H

#include "carmodel.h"
#include <math.h>
#include <map>

// ---- State discretization ----
typedef unsigned int StateIndex;

// Number of discretizations in each dimension.
const static int Y_COUNT = 10;
const static int VX_COUNT = 10;
const static int VY_COUNT = 10;
const static int THETA_COUNT = 30;
// The number of discrete states in the entire space.
const static unsigned int DISCRETE_STATE_COUNT = Y_COUNT*VX_COUNT*VY_COUNT*THETA_COUNT;

// The limits of the discretized space.  States that fall outside of
// these limits will be saturated (i.e., they will be placed on the
// boundary).  Each of the limits are symmetric (e.g., state[STATE_Y]
// should be in the range [-Y_LIMIT, +Y_LIMIT]).
const static double Y_LIMIT = 20;
const static double VX_LIMIT = 10;
const static double VY_LIMIT = 6;
const static double THETA_LIMIT = M_PI;

// ---- Action discretization ----
typedef unsigned short ActionIndex;

// Number of different discrete actions one can take for the two
// controls.  The two actions can be combined into one ActionIndex
// which can take STEERING_COUNT*WHEEL_VEL_COUNT possible values.
const static int STEERING_COUNT = 7;
const static int WHEEL_VEL_COUNT = 5;
// The total possible number of discrete actions.
const static unsigned int DISCRETE_ACTION_COUNT = STEERING_COUNT*WHEEL_VEL_COUNT;

// These arrays hold the continuous values for each discrete action
// that we're allowed to take in the discrete MDP.  So, e.g., when
// taking a discrete steering action 'a0', we'll use a steering angle
// of STEERING_ANGLES[a0] in our simulator call.  Alternatively, we
// could choose an integer in [0, DISCRETE_ACTION_COUNT-1] (inclusive)
// and use undiscretizeAction() to find the equivalent continuous
// action for use in the simulator.
const static double STEERING_ANGLES[STEERING_COUNT] = 
  { -MAX_STEERING, -MAX_STEERING*0.5,  -MAX_STEERING*0.25,
    0, MAX_STEERING*0.25, MAX_STEERING*0.5, MAX_STEERING };
const static double WHEEL_VELOCITIES[WHEEL_VEL_COUNT] = { 0, 1, 3, 5, 10 };

// Returns a single index that represents a discrete action.  This
// index is created from the two discrete actions given for the
// steering angle and the wheel velocity (i.e., the indices of the
// continuous action stored in STEERING_ANGLES and WHEEL_VELOCITIES).
ActionIndex getActionIndex(int steeri, int wheelveli);

// Discretizes a continuous action.  This will find the nearest
// discrete action listed in STEERING_ANGLES, and WHEEL_VELOCITIES to
// the continuous action 'a', and then return its discrete action
// index used for the MDP.
ActionIndex discretizeAction(const double a[ACTION_SIZE]);

// "Undiscretizes" an action.  This takes in the discrete (MDP) action
// and computes the continuous action represented by it, storing it in
// the array 'a'.
void undiscretizeAction(ActionIndex action, double a[ACTION_SIZE]);

// Returns a single index that represents a discrete state.  Each
// parameter is a discretized value of one of the state dimensions.
// So, e.g., yi is a discretized version of state[STATE_Y], and yi
// must be in the range [0, Y_COUNT-1] (inclusive).  You should use
// discretizeState() to convert a continous state (used by the
// simulator) into a discrete state index -- that function will call
// this routine for you.
StateIndex getStateIndex(int yi, int vxi, int vyi, int thetai);

// Discretizes a continous state.  The simulator uses continuous
// states, and this is often the most convenient state format to work
// with.  The MDP is expressed in terms of discrete state values, however.
// This function will return the discrete state that corresponds to the
// given continuous state.
StateIndex discretizeState(const double state[STATE_SIZE]);

// Takes a state index, as generated by discretizeState and finds the
// continous state that lies in the center of the discretized state.
void undiscretizeState(StateIndex discreteState, double state[STATE_SIZE]);

// The Distribution class is a sparse representation of a probability
// distribution over states.  More specifically, it provides a very
// simple mechanism for keeping track of the probability of discrete
// states (as represented by their discrete StateIndex value) without
// keeping an array as large as the entire discretized state space.
// 
// To build a state distribution based on a sample of states, one
// calls the addCount() function for each state in the sample.  This
// will tally the number of times each state is seen, as well as keep
// a running tally of how many states have been seen overall.  The
// "probability" of each state, as returned by a later call to
// probability(), is just the fraction of times that state was seen
// out of all the states added to the distribution.
//
// As an example, if there were two states {Heads, Tails}, and we have
// a sample of states generated by some system, say, {T, H, T, T, H,
// T, T, H}, then we will call addCount() for each of the states in
// the sample.  The total count will be 8.  A call to probability()
// passing state T would return 5/8 -- this is our estimate of the
// probability of state T based on the data entered.
class Distribution {
 public:
  typedef std::map<StateIndex, unsigned int> CountMapType;

  Distribution() : mZ(0) {}

  // Tallies an observation of the given discrete state.
  void addCount(StateIndex dstate) {
    CountMapType::iterator itr = mCounts.find(dstate);
    if (itr == mCounts.end()) mCounts.insert(CountMapType::value_type(dstate,1));
    else itr->second++;
    mZ ++;
  }

  // Returns the estimated probability of the given discrete state.
  // This is the fraction of times that addCount() was called for the
  // given state out of all of the states tallied.
  double probability(StateIndex dstate) const {
    CountMapType::const_iterator itr = mCounts.find(dstate);
    if (itr == mCounts.end()) return 0;
    return double(itr->second) / double(mZ);
  }

  // This returns the total number of samples tallied by this
  // distribution.  You might check this to verify that you have
  // enough data in your distribution to get a good probability
  // estimate.
  unsigned int getSampleCount() const { return mZ; }


  // These return iterators into the internal map of points.  This is
  // used by expectedValue() in main.cpp to compute the necessary
  // expectation quickly.  You do not need to use these.
  CountMapType::const_iterator mapBegin() const { return mCounts.begin(); }
  CountMapType::const_iterator mapEnd() const { return mCounts.end(); }

 private:
  unsigned int mZ;
  CountMapType mCounts;
};

// The transition model is simply a collection of state distributions,
// indexed by a state-action pair.  For a given state, s0, and action,
// a, the TransitionModel stores a Distribution object to keep track
// of the probability of reaching a particular state, s1, given that
// you started in s0 and took action a.  Russel&Norvig denote this
// probability value as T(s0,a,s1), but it is often written
// P_{s0,a}(s1), making clear that the model is a collection of
// distributions, indexed by 's0' and 'a'.
//
// The functions in this class are the same as those in the
// Distribution class, except that the first two parameters are the
// state and action that specify which distribution we're dealing
// with.  E.g., addCount(s0,a,s1) will call addCount(s1) for the
// distribution corresponding to the pair {s0,a}.
class TransitionModel {
public:

  // Tallies an observation of 'state1', given that we started in
  // state 'state0' and took action 'action'.
  void addCount(StateIndex state0,
                ActionIndex action,
                StateIndex state1) {
    mDist[state0][action].addCount(state1);
  }

  // Returns the probability of ending up in state 'state1', given
  // that you started in 'state0' and took action 'action'.
  double probability(StateIndex state0,
                     ActionIndex action,
                     StateIndex state1) const {
    const Distribution* dist = getDistribution(state0, action);
    if (!dist) return 0;
    return dist->probability(state1);
  }

  // Returns the distribution object for a given state-action pair.
  // If that state-action pair has never been observed before, then no
  // distribution will be available, and this function returns 0.
  const Distribution* getDistribution(StateIndex state0,
                                      ActionIndex action) const {
    StateActionDistMapType::const_iterator itr1 = mDist.find(state0);
    if (itr1 == mDist.end()) return 0;
    ActionDistMapType::const_iterator itr2 = itr1->second.find(action);
    if (itr2 == itr1->second.end()) return 0;
    return &itr2->second;
  }

private:
  typedef std::map<ActionIndex, Distribution> ActionDistMapType;
  typedef std::map<StateIndex, ActionDistMapType> StateActionDistMapType;

  StateActionDistMapType mDist;
};

#endif /* TRANSITION_MODEL_H */

